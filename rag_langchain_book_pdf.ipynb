{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG) with LangChain, Pinecone, and ChromaDB for book.pdf\n",
    "\n",
    "This notebook demonstrates **Retrieval-Augmented Generation (RAG)** using **LangChain** with **Pinecone** and **ChromaDB** as vector databases, processing a user-provided PDF document (`book.pdf`). The RAG system extracts text from the PDF, chunks it, creates embeddings, stores them in both vector databases, and answers queries based on the document content. It also compares the performance of Pinecone and ChromaDB.\n",
    "\n",
    "## Objectives\n",
    "- Extract text from `book.pdf`\n",
    "- Chunk the text and create vector embeddings\n",
    "- Store embeddings in Pinecone and ChromaDB\n",
    "- Set up RAG pipelines for question answering\n",
    "- Compare Pinecone and ChromaDB performance\n",
    "\n",
    "## Prerequisites\n",
    "- `book.pdf` in the working directory\n",
    "- Pinecone account and API key (free tier available)\n",
    "- Ollama installed locally with a model (e.g., `llama3`)\n",
    "- Required libraries: `langchain`, `pinecone-client`, `chromadb`, `pypdf`, `sentence-transformers`, `langchain-community`, `langchain-ollama`\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "Install and import the necessary libraries. Replace the Pinecone API key and environment with your credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU langchain langchain-community langchain-ollama pinecone-client chromadb sentence-transformers pypdf\n",
    "\n",
    "import os\n",
    "import pinecone\n",
    "import chromadb\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Pinecone, Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_ollama import OllamaLLM\n",
    "import time\n",
    "\n",
    "# Set Pinecone API key and environment\n",
    "os.environ['PINECONE_API_KEY'] = 'your-pinecone-api-key'  # Replace with your Pinecone API key\n",
    "PINECONE_ENV = 'your-pinecone-environment'  # Replace with your Pinecone environment (e.g., 'us-west1-gcp')\n",
    "\n",
    "# Initialize Pinecone\n",
    "pinecone.init(api_key=os.environ['PINECONE_API_KEY'], environment=PINECONE_ENV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load and Preprocess book.pdf\n",
    "\n",
    "We'll use `PyPDFLoader` to extract text from `book.pdf` and split it into chunks suitable for retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PDF\n",
    "loader = PyPDFLoader('book.pdf')\n",
    "documents = loader.load()\n",
    "\n",
    "# Split into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Number of document chunks: {len(docs)}\")\n",
    "print(\"Sample chunk:\")\n",
    "print(docs[0].page_content[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Embeddings\n",
    "\n",
    "We'll use Hugging Face's `all-MiniLM-L6-v2` model to create 384-dimensional embeddings for the document chunks, compatible with both Pinecone and ChromaDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Hugging Face embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')\n",
    "\n",
    "# Test embedding\n",
    "sample_text = \"This is a test sentence from the book.\"\n",
    "sample_embedding = embeddings.embed_query(sample_text)\n",
    "print(f\"Embedding dimension: {len(sample_embedding)}\")\n",
    "print(f\"Sample embedding (first 5 values): {sample_embedding[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Store Embeddings in Pinecone\n",
    "\n",
    "We'll create a Pinecone index, store the document embeddings, and set up a LangChain vector store for retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pinecone index\n",
    "index_name = 'book-rag'\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    pinecone.create_index(index_name, dimension=384, metric='cosine')\n",
    "\n",
    "# Initialize Pinecone index\n",
    "index = pinecone.Index(index_name)\n",
    "\n",
    "# Store documents in Pinecone\n",
    "pinecone_store = Pinecone.from_documents(docs, embeddings, index_name=index_name)\n",
    "\n",
    "print(f\"Pinecone index '{index_name}' created and populated with {len(docs)} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Store Embeddings in ChromaDB\n",
    "\n",
    "We'll create a ChromaDB collection, store the document embeddings, and set up a LangChain vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ChromaDB client\n",
    "chroma_client = chromadb.PersistentClient(path='./chroma_db')\n",
    "\n",
    "# Store documents in ChromaDB\n",
    "chroma_store = Chroma.from_documents(\n",
    "    docs,\n",
    "    embeddings,\n",
    "    client=chroma_client,\n",
    "    collection_name='book-rag',\n",
    "    persist_directory='./chroma_db'\n",
    ")\n",
    "\n",
    "print(f\"ChromaDB collection 'book-rag' created and populated with {len(docs)} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Set Up RAG Pipeline\n",
    "\n",
    "We'll use LangChain's `RetrievalQA` chain with an Ollama-hosted LLM (`llama3`) to create RAG pipelines for both Pinecone and ChromaDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM (Ollama with llama3)\n",
    "llm = OllamaLLM(model='llama3')\n",
    "\n",
    "# Create RetrievalQA chains\n",
    "pinecone_qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type='stuff',\n",
    "    retriever=pinecone_store.as_retriever(search_kwargs={'k': 3})\n",
    ")\n",
    "\n",
    "chroma_qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type='stuff',\n",
    "    retriever=chroma_store.as_retriever(search_kwargs={'k': 3})\n",
    ")\n",
    "\n",
    "print(\"RAG pipelines initialized for Pinecone and ChromaDB.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Test RAG Pipelines\n",
    "\n",
    "We'll test both RAG pipelines with sample queries relevant to the content of `book.pdf`. Since I don't know the exact content of your PDF, I'll provide generic queries that you can modify based on your document's topics. We'll measure response time and compare outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample queries (modify based on your book.pdf content)\n",
    "queries = [\n",
    "    \"What is the main topic of the book?\",\n",
    "    \"Summarize the key points from the first chapter.\",\n",
    "    \"What are some examples mentioned in the book?\"\n",
    "]\n",
    "\n",
    "# Test and compare\n",
    "for query in queries:\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    \n",
    "    # Pinecone\n",
    "    start_time = time.time()\n",
    "    pinecone_result = pinecone_qa.run(query)\n",
    "    pinecone_time = time.time() - start_time\n",
    "    print(f\"Pinecone Response (Time: {pinecone_time:.2f}s):\")\n",
    "    print(pinecone_result)\n",
    "    \n",
    "    # ChromaDB\n",
    "    start_time = time.time()\n",
    "    chroma_result = chroma_qa.run(query)\n",
    "    chroma_time = time.time() - start_time\n",
    "    print(f\"ChromaDB Response (Time: {chroma_time:.2f}s):\")\n",
    "    print(chroma_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Comparison of Pinecone and ChromaDB\n",
    "\n",
    "### Pinecone\n",
    "- **Pros**:\n",
    "  - Fully managed, scalable vector database\n",
    "  - Optimized for real-time similarity search\n",
    "  - Easy integration with LangChain\n",
    "  - Ideal for production with high throughput\n",
    "- **Cons**:\n",
    "  - Requires an account and API key; free tier limited to 100,000 vectors\n",
    "  - Costly for large-scale use\n",
    "- **Performance**: Typically faster for cloud-based retrieval (~500ms for small indexes)\n",
    "\n",
    "### ChromaDB\n",
    "- **Pros**:\n",
    "  - Open-source and free\n",
    "  - Easy to set up locally or in Docker\n",
    "  - Persists data locally\n",
    "  - Great for prototyping and development\n",
    "- **Cons**:\n",
    "  - Self-hosted, requiring infrastructure management\n",
    "  - Less optimized for large-scale production\n",
    "- **Performance**: Slightly slower for large datasets; performance depends on local hardware\n",
    "\n",
    "## Recommendations\n",
    "- **Use Pinecone** for production-grade applications requiring scalability and managed infrastructure.\n",
    "- **Use ChromaDB** for prototyping, local testing, or projects where self-hosting is preferred.\n",
    "\n",
    "## Notes\n",
    "- If `book.pdf` is large, adjust `chunk_size` and `chunk_overlap` to balance retrieval accuracy and performance.\n",
    "- If the PDF contains scanned images, use an OCR tool like `pytesseract` (let me know if you need help with this).\n",
    "- Response times depend on network latency (Pinecone), hardware (ChromaDB), and LLM performance (Ollama).\n",
    "\n",
    "## Cleanup\n",
    "\n",
    "Delete the Pinecone index and ChromaDB collection to free resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup Pinecone\n",
    "if index_name in pinecone.list_indexes():\n",
    "    pinecone.delete_index(index_name)\n",
    "    print(f\"Deleted Pinecone index '{index_name}'.\")\n",
    "\n",
    "# Cleanup ChromaDB\n",
    "chroma_client.delete_collection('book-rag')\n",
    "print(\"Deleted ChromaDB collection 'book-rag'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation\n",
    "\n",
    "- **PDF Loading and Splitting**: Used `PyPDFLoader` to extract text from `book.pdf` and `RecursiveCharacterTextSplitter` to create manageable chunks.\n",
    "- **Embeddings**: Employed Hugging Face's `all-MiniLM-L6-v2` for 384-dimensional embeddings.\n",
    "- **Pinecone**: Created a cloud-based index, stored embeddings, and set up a `Pinecone` vector store.\n",
    "- **ChromaDB**: Created a local collection, stored embeddings, and set up a `Chroma` vector store.\n",
    "- **RAG Pipeline**: Used `RetrievalQA` with an Ollama-hosted `llama3` LLM to answer queries, retrieving the top-3 relevant chunks.\n",
    "- **Comparison**: Evaluated Pinecone and ChromaDB based on setup, performance, and use cases.\n",
    "\n",
    "## Next Steps\n",
    "- **Modify Queries**: Replace the sample queries with ones specific to `book.pdf` content (e.g., key concepts, chapters, or examples).\n",
    "- **Handle Large PDFs**: If `book.pdf` is large, test with smaller `chunk_size` (e.g., 500) or increase `k` in `search_kwargs`.\n",
    "- **OCR for Scanned PDFs**: If the PDF is scanned, install `pytesseract` and use `pdf2image` for text extraction (I can provide a modified notebook).\n",
    "- **Advanced RAG**: Add prompt templates, hybrid search, or reranking for better results.\n",
    "- **Deployment**: Create a UI with Chainlit or deploy the RAG system as an API.\n",
    "\n",
    "If you encounter issues (e.g., PDF extraction fails, Pinecone setup errors, or specific content queries), let me know, and I can provide tailored solutions!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}